# -*- coding: utf-8 -*-
"""the main one learning from data roberta-base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pGW8Bl8SR3B7vnR2s3GUz2ekyTCeS566
"""

!pip install transformers torch

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from torch.utils.data import Dataset
# Load cleaned data
train_data = pd.read_csv('cleaned_train.csv')
dev_data = pd.read_csv('cleaned_dev.csv')
data = pd.concat([train_data, dev_data])
# Map labels to binary format
data['label'] = data['label'].apply(lambda x: 1 if x == 'OFF' else 0)
# Prepare full training data without splitting
train_texts = data['cleaned_tweet'].tolist()
train_labels = data['label'].tolist()

class TweetDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors="pt"
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer for roberta-base
tokenizer = AutoTokenizer.from_pretrained('roberta-base')
# Create Dataset object for training
train_dataset = TweetDataset(train_texts, train_labels, tokenizer)

from transformers import AutoModelForSequenceClassification

# Load pretrained RoBERTa model for binary classification
model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=2)
print(model)

training_args = TrainingArguments(
    output_dir='./results',           # Output directory for model and logs
    evaluation_strategy="epoch",      # Evaluate at the end of each epoch
    save_strategy="epoch",            # Save checkpoint at each epoch
    learning_rate=2e-5,               # Learning rate for fine-tuning
    per_device_train_batch_size=8,    # Batch size for training
    per_device_eval_batch_size=8,     # Batch size for evaluation
    num_train_epochs=3,               # Number of training epochs
    weight_decay=0.01,                # Weight decay for regularization
    logging_dir='./logs',             # Log directory
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
    }

from transformers import Trainer

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=train_dataset,  # Replace with validation set if available
    compute_metrics=compute_metrics
)
# Fine-tune the model
trainer.train()

# Load test set
test_data = pd.read_csv('cleaned_test.csv')  # Replace with your actual test file name
test_texts = test_data['cleaned_tweet'].tolist()
test_labels = test_data['label'].apply(lambda x: 1 if x == 'OFF' else 0).tolist()

# Create test dataset
test_dataset = TweetDataset(test_texts, test_labels, tokenizer)
# Generate predictions for test set
preds_output = trainer.predict(test_dataset)
y_pred = preds_output.predictions.argmax(axis=1)
from sklearn.metrics import classification_report

# Print classification report for the test set
print("Classification Report (Test Set):")
print(classification_report(test_labels, y_pred, target_names=['NOT', 'OFF']))

# Convert `y_true` to integer format
y_true = test_data['label'].apply(lambda x: 1 if x == 'OFF' else 0).values
from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred, labels=[0, 1])

# Display results
tn, fp, fn, tp = conf_matrix.ravel()
print("True Negatives (NOT correctly classified as NOT):", tn)
print("False Positives (NOT misclassified as OFF):", fp)
print("False Negatives (OFF misclassified as NOT):", fn)
print("True Positives (OFF correctly classified as OFF):", tp)

# Filter and collect misclassified offensive tweets (False Negatives)
misclassified_off = test_data[(test_data['label'] == 'OFF') & (y_pred == 0)]
misclassified_texts = misclassified_off['cleaned_tweet'].tolist()

# Verify the count of misclassified tweets
print(f"Number of misclassified offensive tweets: {len(misclassified_texts)}")

# Save misclassified texts to a text file in Colab
with open('misclassified_texts.txt', 'w') as f:
    for tweet in misclassified_texts:
        f.write(f"{tweet}\n")
from google.colab import files
files.download('misclassified_texts.txt')

import nltk
nltk.download('punkt')

# Now, run the previous tokenization and POS tagging code again

# Load misclassified tweets from the uploaded file
with open('misclassified_texts.txt', 'r') as f:
    misclassified_texts = f.readlines()

# Remove any newline characters from each tweet
misclassified_texts = [tweet.strip() for tweet in misclassified_texts]
import nltk
from nltk import pos_tag, word_tokenize
nltk.download('averaged_perceptron_tagger')

# Tokenize and POS tag each misclassified tweet
pos_tags = []
for tweet in misclassified_texts:
    tokens = word_tokenize(tweet)
    tags = pos_tag(tokens)
    pos_tags.extend(tags)  # Collect POS tags for each word in misclassified tweets

from collections import Counter

# Expanded list of common curse words and offensive terms
curse_words = [
    "damn", "hell", "hate", "stupid", "idiot", "freak", "crap", "bastard",
    "suck", "douche", "moron", "dumb","dumber" ,"jerk", "loser", "scumbag", "screw",
    "piss", "slut", "whore", "trash", "filth", "ass", "bitch", "shit", "fuck",
    "bullshit", "pissed", "dick", "prick", "cunt", "motherfucker", "bastards",
    "freaking", "asshole", "retard", "lame", "screwed", "hater", "hateful","nigga", "freaking", "asshole", "retard", "lame", "dirty", "clown", "ugly", "diseased", "nauseous", "hypocrite", "angry",
    "crazy", "disgusting", "racist", "scam", "scum", "deplorable", "terrorist", "attack",
    "liar", "corrupt", "deceiver", "feminazi", "nazi", "antifa", "radical", "hypocrisy",
    "fascist", "freak", "leftist", "communist", "dems", "socialist", "skeezy", "toxic",
    "sickening"
]

# Filter and count occurrences of these curse words in misclassified tweets
curses_in_misclassified = [word.lower() for word, tag in pos_tags if word.lower() in curse_words]
curse_word_counts = Counter(curses_in_misclassified)

# Count adjectives in misclassified tweets
adjectives = [word.lower() for word, tag in pos_tags if tag == 'JJ']  # JJ is the POS tag for adjectives
adjective_counts = Counter(adjectives)

# Display the results
print("Adjective Counts in Misclassified Tweets:", adjective_counts)
print("Curse Word Counts in Misclassified Tweets:", curse_word_counts)

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer
# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Calculate sentiment scores for misclassified tweets
misclassified_sentiments = [sia.polarity_scores(tweet) for tweet in misclassified_texts]
# Display sentiment scores for the first few misclassified tweets
print(misclassified_sentiments[:5])

# Extract correctly classified offensive tweets (True Positives)
correctly_classified_off = test_data[(test_data['label'] == 'OFF') & (y_pred == 1)]
correctly_classified_texts = correctly_classified_off['cleaned_tweet'].tolist()
# Calculate sentiment scores for correctly classified tweets
correctly_classified_sentiments = [sia.polarity_scores(tweet) for tweet in correctly_classified_texts]
# Display sentiment scores for the first few correctly classified offensive tweets
print(correctly_classified_sentiments[:5])

import numpy as np

# Average sentiment scores for misclassified tweets
misclassified_neg = np.mean([score['neg'] for score in misclassified_sentiments])
misclassified_neu = np.mean([score['neu'] for score in misclassified_sentiments])
misclassified_pos = np.mean([score['pos'] for score in misclassified_sentiments])

# Average sentiment scores for correctly classified tweets
correct_neg = np.mean([score['neg'] for score in correctly_classified_sentiments])
correct_neu = np.mean([score['neu'] for score in correctly_classified_sentiments])
correct_pos = np.mean([score['pos'] for score in correctly_classified_sentiments])

# Display results
print("Average Sentiment Scores for Misclassified Offensive Tweets:")
print(f"Negative: {misclassified_neg}, Neutral: {misclassified_neu}, Positive: {misclassified_pos}")

print("\nAverage Sentiment Scores for Correctly Classified Offensive Tweets:")
print(f"Negative: {correct_neg}, Neutral: {correct_neu}, Positive: {correct_pos}")

import pandas as pd

# Load your dataset (assuming 'cleaned_test.csv' for example)
data = pd.read_csv('cleaned_dev.csv')

# Calculate the length of each tweet
data['tweet_length'] = data['cleaned_tweet'].apply(len)

# Calculate the percentage of tweets under 250 characters
under_250 = data[data['tweet_length'] < 250]
percentage_under_250 = (len(under_250) / len(data)) * 100

print(f"Percentage of tweets under 250 characters: {percentage_under_250:.2f}%")